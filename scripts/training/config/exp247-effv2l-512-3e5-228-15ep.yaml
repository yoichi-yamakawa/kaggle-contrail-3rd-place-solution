# general settings
exp_type: starter
debug: True

# phase: train
n_folds: 5
fold_index: &fold_index 0
max_epochs: &max_epochs 15

# cpu/gpu
gpus: 1
sync_batchnorm: False
precision: 32
num_workers: 16

# dataset
train_batch_size: 4
accumulate_grad_batches: 1 # effective batch size = train_batch_size * accumulate_grad_batches * GPU_NUM
valid_batch_size: 16
test_batch_size: 16
train_dataset_dir: p20-contrail/
train_image_dir: p20-contrail/train/
valid_image_dir: p20-contrail/valid/

test_dataset_dir: p20-contrail/test/
test_image_dir:  p20-contrail/test/
dataset_config: 
  name: ContrailDatasetV4
  params:
    get_mask_frame_only: True
    use_individual_mask: True
    normalize_method: mean_std

transform_config:
  name: Sample
  train_transform: 
    name: BaseTransformV8
    transform_config:
      params:
        HorizontalFlip:
          p: 0.0
        VerticalFlip:
          p: 0.0
        ShiftScaleRotate:
          shift_limit: 0.05
          scale_limit: 0.1
          rotate_limit: 20
          p: 0.25
        RandomResizedCrop:
          height: 512
          width: 512
          scale:
            - 0.5
            - 1.0
          ratio:
            - 0.80
            - 1.25
          p: 0.3
        Resize:
          height: 512
          width: 512
        additional_targets: 1

  valid_transform: 
    name: BaseTransformV1Resize
    transform_config:
      params:
        HorizontalFlip:
          p: 0.0
        VerticalFlip:
          p: 0.0
        Resize:
          height: 512
          width: 512


  test_transform: 
    name: BaseTransformV1Resize
    transform_config:
      params:
        HorizontalFlip:
          p: 0.0
        VerticalFlip:
          p: 0.0
        Resize:
          height: 512
          width: 512

# model
pl_module: BasePLM
model_dir: p20-contrail/output

model_config: 
  name: UNetBaseV1 
  gradient_clip_val: 1.0
  params:
    model_params:
      from_pretrained: True
      pretrained_weight_dir: p20-contrail/output/exp228-effv2l-512-5e4-knpl-f4s-100ep/
      backbone: tf_efficientnetv2_l.in21k_ft_in1k
      in_channel_num: 6
      
monitor: val_kaggle_score
stochastic_weight_avg: False

# loss
loss_config: 
  name: DiceBCEWithLogitsLossV1 
  params:
    mode: binary
    base_weight: 0.5
    aux_weight: 0.5
    base_bce_weight: 0.0
    aux_bce_weight: 0.5

# lr scheduler
# lr scheduler
lr_scheduler_config: 
  name: CosineAnnealingLR 
  lr_scheduler_params: 
    eta_min: 1.0e-7
    T_max: *max_epochs

# optimizer
optimizer_config: 
  name: AdamW
  optimizer_params: 
    lr: 3.0e-5

# logger
logger: wandb